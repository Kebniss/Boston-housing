Question 1:

Accuracy is 61.62%

Question 2:

Accuracy is 78.68%

Question 3:

Accuracy is 79.35%

Question 4:

Accuracy is 80.25%

Question 5:

The finish line positions in a car race. The outcome is the position at which each car ends the race while the features are the overall points they scored that season and/or the previous one, how many times they ended a race in a position and how the cars are tuned (for example what type of wheels are they using).

Question 6:

Cross-validation is a technique used to exploit the totality of the data for both training and testing. It consists in dividing the data in two complementary sets: one for testing and one for training. This operation is repeated several times on different partition of the data. The final result from the training is the mean of the errors obtained with each partition. If this method wasn't used in combination with grid search then the result of grid search would be of finding the combination of parameters that work best with that specific set of data, again, this might not be representative of a more general situation.

Question 7:

max_ depth = 1. As the training set size increases the training error raises to a bias value, this happens because the model is too simple to generalize well a large number of data. Test error rapidly decreases to an almost constant level with the growth of training points, this happens again because the model is too simple and misinterprets the data. Notable the variance on the test error is very low compared to other depths.

Question 8:

The model suffer from high bias when depth is 1 while it suffers from great variance when it is 10.

Question 9:

The training error is invertially proportional to the model depth: this happens because by increasing the complexity the regression fits tighter and tighter around training data. This phenomenon however makes it difficult for the model to generalize to independent data, as can be seen in the evolution of the testing error. The testing error initially decreases because the model gets more complex and it's able to interpret the training data. However when the depth becomes too big the model it's not able to correctly work for the test set and the testing error rises. 
According to the graph I am seeing the best depth is 4 as there is a clear minimum in the testing error.

Question 10:

I run the code ten times. Median value was 6 while the average was 4.6. The average value is consistent with my prediction.

Question 11:

Run the code block below to have your parameter-tuned model make a prediction on the client's home.

Question 12:

In order to make this decision I need to understand what is the error that is applicable to my prediction. The testing mean squared error ranges between 15 and 20, I'll use the higher value so to have a more conservative estimate. An MSE of 20 means an error on the measure of +-4.47 which is roughly 20% of the sum. This means that my prediction has an error bar of overall 40%: the correct price could be 20% higher or 20% lower. The error is pretty wide and I think it's caused by the small test set we are using. That being said i wouldn't use the model to define the exact price of a property, however if I didn't have any other reference I would use it at least to get an idea of what the price could be.